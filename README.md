#  - TOXIC PROMPT Classification

This project focuses on building a transformer-based classification system to detect and flag the **toxic prompts** during customer engagements.

---

## - Objective

To fine-tune a transformer model that can accurately classify prompts as **safe** or **unsafe**, enabling effective **prompt guardrails** in GENAI systems.

---

## - Explanation

All the explanation about the setup, dataset used, model and metrics are clearly given in the notebook.

## - Summary and Discussion

### 1. Results

- Accuracy: ~96.8%
- F1-score: ~0.84
- Precision/Recall show strong balance for binary toxic content classification.


```bash
git clone https://github.com/Prathama-1/TOXIC-PROMPT-Classification.git
cd TOXIC-PROMPT-Classification

